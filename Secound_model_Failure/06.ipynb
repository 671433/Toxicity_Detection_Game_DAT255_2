{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1db4aa32-b184-419a-a262-80d4d6f4e2a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1db4aa32-b184-419a-a262-80d4d6f4e2a4",
    "outputId": "f02843fa-96a1-462b-bb15-79a7d8acf20d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a3704cc-a214-435e-ace6-e1b87d839c05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6a3704cc-a214-435e-ace6-e1b87d839c05",
    "outputId": "acd48e1e-e298-40fe-c04a-e26f5a93e510"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\", 'toxicity': 0.0, 'severe_toxicity': 0.0, 'obscene': 0.0, 'threat': 0.0, 'insult': 0.0, 'identity_attack': 0.0, 'sexual_explicit': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Load civil_comments dataset\n",
    "dataset = load_dataset(\"google/civil_comments\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bce068b-0ac8-4c10-bf69-bab2f4e311ae",
   "metadata": {
    "id": "2bce068b-0ac8-4c10-bf69-bab2f4e311ae"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "labels = df[df.columns[2:]].values\n",
    "\n",
    "# texts\n",
    "texts = df['text'].astype(str).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d83bcc9c-f455-4234-9a19-6f3f284bbf4a",
   "metadata": {
    "id": "d83bcc9c-f455-4234-9a19-6f3f284bbf4a"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "df['text'] = df['text'].map(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bec88fe-9a76-498c-a566-7bd3f45573ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7bec88fe-9a76-498c-a566-7bd3f45573ed",
    "outputId": "a6a886f9-a2ec-4ef3-de79-4cd3f247c58a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "316011a5-9fe0-4c76-a7ed-0642ee1d7bb9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "316011a5-9fe0-4c76-a7ed-0642ee1d7bb9",
    "outputId": "8a352712-af82-4723-9e98-3482d8c0a4d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of values ​​that equal an empty string: 339\n"
     ]
    }
   ],
   "source": [
    "is_empty_string = df['text'] == ''\n",
    "count_empty_string = is_empty_string.sum()\n",
    "print(\"Number of values ​​that equal an empty string:\", count_empty_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22ff1087-77cf-436e-af90-e6868e0a7d02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22ff1087-77cf-436e-af90-e6868e0a7d02",
    "outputId": "29bf3fd3-aeac-4e0e-a225-e8f65a75bacc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame containing empty text:\n",
      "         text  toxicity  severe_toxicity  obscene  threat  insult  \\\n",
      "276                0.1              0.0      0.0     0.0     0.0   \n",
      "316                0.0              0.0      0.0     0.0     0.0   \n",
      "10598              0.0              0.0      0.0     0.0     0.0   \n",
      "20487              0.0              0.0      0.0     0.0     0.0   \n",
      "33283              0.0              0.0      0.0     0.0     0.0   \n",
      "...      ...       ...              ...      ...     ...     ...   \n",
      "1766576            0.0              0.0      0.0     0.0     0.0   \n",
      "1767014            0.0              0.0      0.0     0.0     0.0   \n",
      "1785002            0.0              0.0      0.0     0.0     0.0   \n",
      "1795472            0.0              0.0      0.0     0.0     0.0   \n",
      "1796225            0.0              0.0      0.0     0.0     0.0   \n",
      "\n",
      "         identity_attack  sexual_explicit  \n",
      "276                  0.0              0.1  \n",
      "316                  0.0              0.0  \n",
      "10598                0.0              0.0  \n",
      "20487                0.0              0.0  \n",
      "33283                0.0              0.0  \n",
      "...                  ...              ...  \n",
      "1766576              0.0              0.0  \n",
      "1767014              0.0              0.0  \n",
      "1785002              0.0              0.0  \n",
      "1795472              0.0              0.0  \n",
      "1796225              0.0              0.0  \n",
      "\n",
      "[339 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "empty_string_df = df[is_empty_string]\n",
    "print(\"DataFrame containing empty text:\\n\", empty_string_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dabd4f95-5bd8-4480-bb6f-107f11e2f0aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dabd4f95-5bd8-4480-bb6f-107f11e2f0aa",
    "outputId": "264d71a7-f6fc-4c37-8f4c-6abfc5509529"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with empty string in 'text': 339\n"
     ]
    }
   ],
   "source": [
    "empty_string_rows = df[df['text'] == '']\n",
    "print(f\"Number of rows with empty string in 'text': {len(empty_string_rows)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bd151cd-943b-4cc6-a806-6b8e25446105",
   "metadata": {
    "id": "4bd151cd-943b-4cc6-a806-6b8e25446105"
   },
   "outputs": [],
   "source": [
    "# Remove the rows where text has an empty string\n",
    "df = df[df['text'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d93f2ed-7e86-4a4c-b96d-78b9b2e383c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9d93f2ed-7e86-4a4c-b96d-78b9b2e383c7",
    "outputId": "902bc046-8301-4669-e3a7-ccc5ca31110f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with empty string in 'text': 0\n"
     ]
    }
   ],
   "source": [
    "empty_string_rows = df[df['text'] == '']\n",
    "print(f\"Number of rows with empty string in 'text': {len(empty_string_rows)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6PfjGbY5TnXZ",
   "metadata": {
    "id": "6PfjGbY5TnXZ"
   },
   "outputs": [],
   "source": [
    "texts = df['text'].astype(str).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GcqajAdeWrDD",
   "metadata": {
    "id": "GcqajAdeWrDD"
   },
   "outputs": [],
   "source": [
    "filtered_texts = []\n",
    "filtered_labels = []\n",
    "\n",
    "for text, label in zip(texts, labels):\n",
    "    if text.strip():  \n",
    "        filtered_texts.append(text)\n",
    "        filtered_labels.append(label)\n",
    "\n",
    "texts = filtered_texts\n",
    "labels = filtered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bf2937b-3000-40e1-b4c6-baacb558de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save clean dataset \n",
    "import numpy as np\n",
    "\n",
    "# NumPy arrays\n",
    "texts = np.array(texts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# save\n",
    "np.save('clean_texts.npy', texts)\n",
    "np.save('clean_labels.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc4d6b7-37a2-475f-8ce4-eb633cc76f01",
   "metadata": {
    "id": "edc4d6b7-37a2-475f-8ce4-eb633cc76f01"
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 12.5 GiB for an array with shape (1804512,) and data type <U1855",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 9\u001b[0m\n\u001b[0;32m      2\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TextVectorization(\n\u001b[0;32m      3\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20000\u001b[39m,\n\u001b[0;32m      4\u001b[0m     output_sequence_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m,  \u001b[38;5;66;03m# قلل الطول لتقليل استهلاك الذاكرة\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     output_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# تدريب الـ vectorizer على النصوص\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madapt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive - Høgskulen på Vestlandet\\Dokumenter\\3 År\\DAT255 Deep learning\\toxicity2\\toxicityPython39\\lib\\site-packages\\keras\\src\\layers\\preprocessing\\text_vectorization.py:423\u001b[0m, in \u001b[0;36mTextVectorization.adapt\u001b[1;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_state(batch)\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 423\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mensure_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    425\u001b[0m         \u001b[38;5;66;03m# A plain list of strings\u001b[39;00m\n\u001b[0;32m    426\u001b[0m         \u001b[38;5;66;03m# is treated as as many documents\u001b[39;00m\n\u001b[0;32m    427\u001b[0m         data \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(data, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\OneDrive - Høgskulen på Vestlandet\\Dokumenter\\3 År\\DAT255 Deep learning\\toxicity2\\toxicityPython39\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:37\u001b[0m, in \u001b[0;36mensure_tensor\u001b[1;34m(inputs, dtype)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mbackend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mis_tensor(inputs):\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;66;03m# Plain `np.asarray()` conversion fails with PyTorch.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mconvert_to_numpy(inputs)\n\u001b[1;32m---> 37\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n\u001b[0;32m     39\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(inputs, dtype)\n",
      "File \u001b[1;32m~\\OneDrive - Høgskulen på Vestlandet\\Dokumenter\\3 År\\DAT255 Deep learning\\toxicity2\\toxicityPython39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\OneDrive - Høgskulen på Vestlandet\\Dokumenter\\3 År\\DAT255 Deep learning\\toxicity2\\toxicityPython39\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:96\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to an `EagerTensor`.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03mNote that this function could return cached copies of created constants for\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m  TypeError: if `dtype` is not compatible with the type of t.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     93\u001b[0m   \u001b[38;5;66;03m# Make a copy explicitly because the EagerTensor might share the underlying\u001b[39;00m\n\u001b[0;32m     94\u001b[0m   \u001b[38;5;66;03m# memory with the input array. Without this copy, users will be able to\u001b[39;00m\n\u001b[0;32m     95\u001b[0m   \u001b[38;5;66;03m# modify the EagerTensor after its creation by changing the input array.\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m   value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ops\u001b[38;5;241m.\u001b[39mEagerTensor):\n\u001b[0;32m     98\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 12.5 GiB for an array with shape (1804512,) and data type <U1855"
     ]
    }
   ],
   "source": [
    "# Create a vectorizer\n",
    "vectorizer = TextVectorization(\n",
    "    max_tokens=20000,\n",
    "    output_sequence_length=300,  \n",
    "    output_mode='int'\n",
    ")\n",
    "\n",
    "# Training the vectorizer on texts\n",
    "vectorizer.adapt(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aa3330-8889-434b-9cb7-8b48d66482e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9aa3330-8889-434b-9cb7-8b48d66482e1",
    "outputId": "1a5d2317-e1b0-4ea2-e001-1e70e935a4fe"
   },
   "outputs": [],
   "source": [
    "vectorizer('Hello, Deep learing is non eazy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a43bcf-f434-45ae-888d-dea771037fdd",
   "metadata": {
    "id": "c6a43bcf-f434-45ae-888d-dea771037fdd"
   },
   "outputs": [],
   "source": [
    "# Convert data to Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((texts, labels))\n",
    "\n",
    "# Apply vectorization\n",
    "dataset = dataset.map(lambda x, y: (vectorizer(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Data organization\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(1804874)\n",
    "dataset = dataset.batch(32)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3425c962-9f07-4002-829f-d74206bed777",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "3425c962-9f07-4002-829f-d74206bed777",
    "outputId": "647d3a6d-9455-45c6-eb78-b22f79917d32"
   },
   "outputs": [],
   "source": [
    "for x_batch, y_batch in dataset.take(1):\n",
    "    print(\"x_batch shape:\", x_batch.shape)\n",
    "    print(\"y_batch shape:\", y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15abb17c-a6a6-48b0-ac07-c6e080f3110a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15abb17c-a6a6-48b0-ac07-c6e080f3110a",
    "outputId": "e4992e9e-7a16-4f02-cea1-1e69587d9437"
   },
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8738b8bd-2d2b-4dae-a4ba-0330f785bba9",
   "metadata": {
    "id": "8738b8bd-2d2b-4dae-a4ba-0330f785bba9"
   },
   "outputs": [],
   "source": [
    "batch_X , batch_Y = dataset.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbda114-4e9b-43ca-93c8-fa3377525390",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cbbda114-4e9b-43ca-93c8-fa3377525390",
    "outputId": "900fa077-2c51-41a1-affe-eab5bf397ac3"
   },
   "outputs": [],
   "source": [
    "batch_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b62074-5413-4c7f-bd4c-3d385092e3aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1b62074-5413-4c7f-bd4c-3d385092e3aa",
    "outputId": "a82a779a-f3ff-4ca1-99d3-3c035a9e493c"
   },
   "outputs": [],
   "source": [
    "batch_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e89f5e-1bee-4cad-b1a1-deefb9f1586d",
   "metadata": {
    "id": "e9e89f5e-1bee-4cad-b1a1-deefb9f1586d"
   },
   "outputs": [],
   "source": [
    "train = dataset.take(int(len(dataset)*.7))\n",
    "val = dataset.skip(int(len(dataset)*.7)).take(int(len(dataset)*.2))\n",
    "test = dataset.skip(int(len(dataset)*.9)).take(int(len(dataset)*.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a8ddd9-02db-4617-b9f9-ff3921fbd568",
   "metadata": {
    "id": "99a8ddd9-02db-4617-b9f9-ff3921fbd568"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, Bidirectional, Dense, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9637264-c040-459a-8096-8a2cf51b59d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9637264-c040-459a-8096-8a2cf51b59d0",
    "outputId": "9332fcd0-7e5b-47a5-b579-15f0e25f1a8e"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(300,)))\n",
    "# Create the embedding layer\n",
    "model.add(Embedding(input_dim=20001, output_dim=32, input_length=300))\n",
    "# Bidirectional LSTM Layer\n",
    "model.add(Bidirectional(LSTM(32, activation='tanh')))\n",
    "# Feature extractor Fully connected layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# Final layer\n",
    "model.add(Dense(6, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e400f06-8ffe-4b8f-9e43-7fdc5789f2bc",
   "metadata": {
    "id": "1e400f06-8ffe-4b8f-9e43-7fdc5789f2bc"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='BinaryCrossentropy', optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbbc41a-67f3-4d56-b070-17b84114d1ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "ccbbc41a-67f3-4d56-b070-17b84114d1ce",
    "outputId": "f1046c62-54dc-41f1-ec07-1d7a460bd0f3"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa5b40e-44cf-4668-89e4-aa01c9c19cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=4,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f16ecda-67e4-464b-98e3-46e43fb057ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4f16ecda-67e4-464b-98e3-46e43fb057ff",
    "outputId": "33c621f1-0949-4568-81e9-163231dab05f"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train, \n",
    "                    epochs=5, \n",
    "                    validation_data=val,\n",
    "                    batch_size=64,\n",
    "                    callbacks=[early_stop]       \n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h2ZH6ppyENzU",
   "metadata": {
    "id": "h2ZH6ppyENzU"
   },
   "outputs": [],
   "source": [
    "model.save('final_model.h5')\n",
    "print(\"model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dbcd56-f261-465e-a686-a76901a88bb3",
   "metadata": {
    "id": "d4dbcd56-f261-465e-a686-a76901a88bb3"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(8,5))\n",
    "pd.DataFrame(history.history).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZDM2TBQgFPWh",
   "metadata": {
    "id": "ZDM2TBQgFPWh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (toxicityPython39)",
   "language": "python",
   "name": "toxicitypython39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
