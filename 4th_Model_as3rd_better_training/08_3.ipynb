{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea459681-7537-477c-84eb-27adee443908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "restored_dataset = load_from_disk(\"cleaned_balanced_civil_comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a9b7e1d-b3d8-42ee-8ac1-949cc64ba229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>sexual_explicit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>haha you guys are a bunch of losers</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ur a shtty comment</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.638095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>its ridiculous that these guys are being calle...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this story gets more ridiculous by the hour an...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>angry trolls misogynists and racists oh my it ...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxicity  \\\n",
       "0                haha you guys are a bunch of losers  0.893617   \n",
       "1                                 ur a shtty comment  0.666667   \n",
       "2  its ridiculous that these guys are being calle...  0.600000   \n",
       "3  this story gets more ridiculous by the hour an...  0.500000   \n",
       "4  angry trolls misogynists and racists oh my it ...  0.500000   \n",
       "\n",
       "   severe_toxicity   obscene  threat    insult  identity_attack  \\\n",
       "0         0.021277  0.000000     0.0  0.872340         0.021277   \n",
       "1         0.047619  0.638095     0.0  0.333333         0.000000   \n",
       "2         0.000000  0.100000     0.1  0.600000         0.000000   \n",
       "3         0.000000  0.000000     0.0  0.300000         0.000000   \n",
       "4         0.000000  0.000000     0.0  0.500000         0.100000   \n",
       "\n",
       "   sexual_explicit  \n",
       "0         0.000000  \n",
       "1         0.009524  \n",
       "2         0.000000  \n",
       "3         0.000000  \n",
       "4         0.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "df = pd.DataFrame(restored_dataset)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae3742e-0d0f-4e9d-adb5-501e5ec235ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null or None values:\n",
      "text               0\n",
      "toxicity           0\n",
      "severe_toxicity    0\n",
      "obscene            0\n",
      "threat             0\n",
      "insult             0\n",
      "identity_attack    0\n",
      "sexual_explicit    0\n",
      "dtype: int64\n",
      "\n",
      "Empty string values in 'text':\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check null or None\n",
    "print(\"Null or None values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check only empty text in the text column\n",
    "print(\"\\nEmpty string values in 'text':\")\n",
    "print((df[\"text\"] == \"\").sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7fbafe7-341c-4e07-84ed-2fa931d878c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lables\n",
    "labels = df[df.columns[1:]].values\n",
    "\n",
    "# texts\n",
    "texts = df['text'].astype(str).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "940ee11a-f398-46bc-a99d-23312b3e5abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(297057,)\n"
     ]
    }
   ],
   "source": [
    "print(type(texts))\n",
    "print(texts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a29be7d0-9265-4696-b5e5-085a64546333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(297057, 7)\n"
     ]
    }
   ],
   "source": [
    "print(type(labels))\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20d3ac60-4c7f-4742-8b75-43d5259e9949",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_texts = []\n",
    "filtered_labels = []\n",
    "\n",
    "for text, label in zip(texts, labels):\n",
    "    if text.strip():  \n",
    "        filtered_texts.append(text)\n",
    "        filtered_labels.append(label)\n",
    "\n",
    "texts = filtered_texts\n",
    "labels = filtered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f217819f-6b01-46b5-a5f1-644468ea3aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS=20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ddcf436-5a63-4889-9588-737699d3f757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vectorizer\n",
    "vectorizer = TextVectorization(\n",
    "    max_tokens=MAX_WORDS,\n",
    "    output_sequence_length=300,  \n",
    "    output_mode='int'\n",
    ")\n",
    "\n",
    "# Vectorizer training on texts\n",
    "vectorizer.adapt(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "728f0150-8c6f-4f1c-8202-cea3e1f5baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# numpy\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split train and temp (val + test)\n",
    "texts_train, texts_temp, labels_train, labels_temp = train_test_split(\n",
    "    texts, labels, test_size=0.3, random_state=42, stratify=labels.argmax(axis=1)\n",
    ")\n",
    "\n",
    "# Split temp into val and test\n",
    "texts_val, texts_test, labels_val, labels_test = train_test_split(\n",
    "    texts_temp, labels_temp, test_size=0.33, random_state=42, stratify=labels_temp.argmax(axis=1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31e25db8-d8d2-4846-bb69-5f18fc286a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save spilt data\n",
    "import numpy as np\n",
    "\n",
    "np.savez_compressed(\n",
    "    'dataset_splits.npz',\n",
    "    texts_train=texts_train,\n",
    "    labels_train=labels_train,\n",
    "    texts_val=texts_val,\n",
    "    labels_val=labels_val,\n",
    "    texts_test=texts_test,\n",
    "    labels_test=labels_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a97ee497-c917-4237-828b-c563bc22af15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def create_dataset(texts, labels):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((texts, labels))\n",
    "    ds = ds.map(lambda x, y: (vectorizer(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.cache().shuffle(10000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# Application to the three groups\n",
    "train = create_dataset(texts_train, labels_train)\n",
    "val = create_dataset(texts_val, labels_val)\n",
    "test = create_dataset(texts_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "367ba54e-0ab2-47f6-8603-8363aca5afda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train positive: 10224\n",
      "Val positive: 2935\n",
      "Test positive: 1446\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Train positive:\", np.sum(labels_train.argmax(axis=1) > 0))\n",
    "print(\"Val positive:\", np.sum(labels_val.argmax(axis=1) > 0))\n",
    "print(\"Test positive:\", np.sum(labels_test.argmax(axis=1) > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7ad696f-3a95-473d-8e4b-bc7f50a3c347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, Bidirectional, Dense, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e98134d3-1a2d-48d4-bf68-ba8f5f1c6dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(300,)))\n",
    "# Create the embedding layer\n",
    "model.add(Embedding(MAX_WORDS+1, 32))\n",
    "# Bidirectional LSTM Layer\n",
    "model.add(Bidirectional(LSTM(32, activation='tanh')))\n",
    "# Feature extractor Fully connected layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# Final layer\n",
    "model.add(Dense(7, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c00b1150-b765-462b-a085-37f4fd2dc67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "import tensorflow as tf\n",
    "\n",
    "# Compile with metrics\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        tf.keras.metrics.AUC(name='auc'),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5c8b780-3160-4e77-9dcc-eb5d70f4ad40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">640,032</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">903</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m640,032\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m16,640\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │           \u001b[38;5;34m8,320\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │          \u001b[38;5;34m33,024\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m32,896\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)                   │             \u001b[38;5;34m903\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">731,815</span> (2.79 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m731,815\u001b[0m (2.79 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">731,815</span> (2.79 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m731,815\u001b[0m (2.79 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39534a7c-6bd3-4b51-824c-5b9b116fbdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=4,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"best_model.keras\", \n",
    "    monitor='val_auc', \n",
    "    save_best_only=True, \n",
    "    mode='max'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "847f4d6b-85a5-4565-ac1c-9210a1fbd20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m6498/6498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1191s\u001b[0m 182ms/step - auc: 0.8356 - loss: 0.2290 - precision: 0.9218 - recall: 0.2548 - val_auc: 0.9036 - val_loss: 0.1875 - val_precision: 0.9707 - val_recall: 0.3611\n",
      "Epoch 2/10\n",
      "\u001b[1m6498/6498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1288s\u001b[0m 198ms/step - auc: 0.9147 - loss: 0.1841 - precision: 0.9743 - recall: 0.3570 - val_auc: 0.9253 - val_loss: 0.1830 - val_precision: 0.9719 - val_recall: 0.3784\n",
      "Epoch 3/10\n",
      "\u001b[1m6498/6498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1359s\u001b[0m 209ms/step - auc: 0.9275 - loss: 0.1757 - precision: 0.9757 - recall: 0.3713 - val_auc: 0.9181 - val_loss: 0.1815 - val_precision: 0.9625 - val_recall: 0.4032\n",
      "Epoch 4/10\n",
      "\u001b[1m6498/6498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1038s\u001b[0m 160ms/step - auc: 0.9347 - loss: 0.1708 - precision: 0.9780 - recall: 0.3820 - val_auc: 0.9216 - val_loss: 0.1833 - val_precision: 0.9655 - val_recall: 0.3895\n",
      "Epoch 5/10\n",
      "\u001b[1m6498/6498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1077s\u001b[0m 166ms/step - auc: 0.9412 - loss: 0.1669 - precision: 0.9793 - recall: 0.3913 - val_auc: 0.9158 - val_loss: 0.1902 - val_precision: 0.9575 - val_recall: 0.4069\n",
      "Epoch 6/10\n",
      "\u001b[1m6498/6498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1062s\u001b[0m 163ms/step - auc: 0.9464 - loss: 0.1638 - precision: 0.9810 - recall: 0.3971 - val_auc: 0.9149 - val_loss: 0.1940 - val_precision: 0.9590 - val_recall: 0.3975\n",
      "Epoch 7/10\n",
      "\u001b[1m6498/6498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1110s\u001b[0m 171ms/step - auc: 0.9502 - loss: 0.1613 - precision: 0.9833 - recall: 0.4062 - val_auc: 0.9119 - val_loss: 0.1990 - val_precision: 0.9422 - val_recall: 0.4082\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train, \n",
    "                    epochs=10, \n",
    "                    validation_data=val,\n",
    "                    batch_size=32,\n",
    "                    callbacks=[early_stop, checkpoint]       \n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e653aa2b-a7b2-4bf2-8de3-05c0a0118ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('best_model.keras')\n",
    "print(\"model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e2b969-9669-4181-bbe9-1395eb42be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "label_names = ['toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit']\n",
    "\n",
    "# Dump val data\n",
    "y_true = []\n",
    "for batch in val:\n",
    "    _, labels = batch\n",
    "    y_true.append(labels)\n",
    "\n",
    "y_true = np.concatenate([y.numpy() for y in y_true], axis=0)\n",
    "\n",
    "# Convert y_true to binary data (0 or 1)\n",
    "y_true_binary = (y_true >= 0.5).astype(int)\n",
    "\n",
    "# Predictions\n",
    "y_true = np.vstack([labels.numpy() for _, labels in val])\n",
    "y_pred_binary = (y_pred >= 0.5).astype(int)\n",
    "\n",
    "# Confusion matrices for each classification\n",
    "for i, label in enumerate(label_names):\n",
    "    print(f\"\\nConfusion Matrix for '{label}':\")\n",
    "    cm = confusion_matrix(y_true_binary[:, i], y_pred_binary[:, i])\n",
    "    print(cm)\n",
    "\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {label}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Full Performance Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "                            y_true_binary,\n",
    "                            y_pred_binary,\n",
    "                            target_names=label_names,\n",
    "                            zero_division=0,\n",
    "                            labels=[0, 1]  \n",
    "                            ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2676775-c729-4b9a-969e-a155c99bc772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy\n",
    "\n",
    "# Performance measure for each classification\n",
    "pre = Precision()\n",
    "re = Recall()\n",
    "acc = BinaryAccuracy()\n",
    "\n",
    "for batch in val:\n",
    "    X_true, y_true = batch\n",
    "    \n",
    "    # Making predictions as tensors\n",
    "    yhat = model(X_true, training=False)\n",
    "    \n",
    "    # Convert values ​​to binary\n",
    "    y_true_binary = tf.cast(y_true >= 0.5, tf.float32)\n",
    "    yhat_binary = tf.cast(yhat >= 0.5, tf.float32)\n",
    "\n",
    "    # Update metrics\n",
    "    pre.update_state(y_true_binary, yhat_binary)\n",
    "    re.update_state(y_true_binary, yhat_binary)\n",
    "    acc.update_state(y_true_binary, yhat_binary)\n",
    "\n",
    "\n",
    "print(f'Precision: {pre.result().numpy()}, Recall: {re.result().numpy()}, Accuracy: {acc.result().numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8f12f6-4848-4bc1-b11f-66b7d0f17c48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (toxicityPython39)",
   "language": "python",
   "name": "toxicitypython39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
